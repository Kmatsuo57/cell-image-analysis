import numpy as np
import tifffile as tiff
import os
from glob import glob
from tensorflow.keras.models import load_model
from stardist.models import StarDist2D
from csbdeep.utils import normalize
from skimage.measure import regionprops, label as skimage_label
from skimage.segmentation import clear_border
from skimage.transform import resize
from skimage import exposure
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from datetime import datetime
import pickle

class ProductionMutantScreening:
    def __init__(self, model_dir):
        self.model_dir = model_dir
        self.load_trained_models()
        
    def load_trained_models(self):
        """è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆé«˜æ„Ÿåº¦ç‰ˆè¿½åŠ ï¼‰"""
        print("Loading trained models...")
        
        # Autoencoderã¨Encoder
        self.autoencoder = load_model(os.path.join(self.model_dir, 'best_autoencoder.keras'))
        self.encoder = load_model(os.path.join(self.model_dir, 'encoder.keras'))
        
        # å‰å‡¦ç†å™¨
        with open(os.path.join(self.model_dir, 'scaler.pkl'), 'rb') as f:
            self.scaler = pickle.load(f)
        with open(os.path.join(self.model_dir, 'pca.pkl'), 'rb') as f:
            self.pca = pickle.load(f)
        
        # ç•°å¸¸æ¤œçŸ¥å™¨ï¼ˆé«˜æ„Ÿåº¦ç‰ˆå«ã‚€ï¼‰
        self.detectors = {}
        detector_files = {
            'extreme_conservative': 'detector_extremeconservative.pkl',
            'ultra_conservative': 'detector_ultraconservative.pkl',
            'super_conservative': 'detector_superconservative.pkl', 
            'very_conservative': 'detector_veryconservative.pkl',
            'conservative': 'detector_conservative.pkl',
            'moderate': 'detector_moderate.pkl'
        }
        
        for name, filename in detector_files.items():
            filepath = os.path.join(self.model_dir, filename)
            if os.path.exists(filepath):
                with open(filepath, 'rb') as f:
                    self.detectors[name] = pickle.load(f)
                print(f"  Loaded {name} detector")
            else:
                print(f"  Warning: {filename} not found")
        
        # StarDist
        self.stardist_model = StarDist2D.from_pretrained('2D_versatile_fluo')
        
        print(f"All models loaded successfully! ({len(self.detectors)} detectors)")
    
    def extract_quality_cells(self, image_path):
        """å“è³ªç®¡ç†ä»˜ãç´°èƒæŠ½å‡ºï¼ˆè¨“ç·´æ™‚ã¨åŒã˜æ¡ä»¶ï¼‰"""
        try:
            image = tiff.imread(image_path)
            
            # ãƒãƒ£ãƒ³ãƒãƒ«åˆ†é›¢
            if image.ndim == 3 and image.shape[-1] >= 3:
                seg_channel = image[..., 2]  # ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”¨
                green_channel = image[..., 1]  # è§£æç”¨
            else:
                seg_channel = image
                green_channel = image
            
            # StarDist ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
            normalized_seg = normalize(seg_channel)
            labels, details = self.stardist_model.predict_instances(normalized_seg)
            
            # å“è³ªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆè¨“ç·´æ™‚ã¨åŒã˜æ¡ä»¶ï¼‰
            height, width = labels.shape
            props = regionprops(labels)
            
            quality_cells = []
            cell_stats = []
            
            for prop in props:
                minr, minc, maxr, maxc = prop.bbox
                
                # å¢ƒç•Œãƒã‚§ãƒƒã‚¯
                if (minr < 10 or minc < 10 or maxr > (height - 10) or maxc > (width - 10)):
                    continue
                
                # ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
                if prop.area < 200 or prop.area > 8000:
                    continue
                
                # å½¢çŠ¶ãƒã‚§ãƒƒã‚¯
                if prop.eccentricity > 0.95:
                    continue
                
                # ç´°èƒç”»åƒæŠ½å‡º
                cell_image = green_channel[minr:maxr, minc:maxc]
                
                # å¼·åº¦ãƒã‚§ãƒƒã‚¯
                cell_mean = np.mean(cell_image)
                cell_std = np.std(cell_image)
                
                if cell_mean < 0.5 or cell_std < 0.1:
                    continue
                
                # è¨“ç·´æ™‚ã¨åŒã˜å‰å‡¦ç†
                cell_image_eq = exposure.equalize_adapthist(cell_image, clip_limit=0.02)
                cell_image_resized = resize(cell_image_eq, (64, 64), anti_aliasing=True)
                
                quality_cells.append(cell_image_resized)
                
                cell_stats.append({
                    'area': prop.area,
                    'eccentricity': prop.eccentricity,
                    'solidity': prop.solidity,
                    'mean_intensity': cell_mean,
                    'std_intensity': cell_std
                })
            
            return quality_cells, cell_stats
            
        except Exception as e:
            print(f"Error processing {image_path}: {e}")
            return [], []
    
    def compute_anomaly_scores(self, cell_images):
        """åŸºæœ¬çš„ç•°å¸¸ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        if len(cell_images) == 0:
            return {}
        
        X = np.expand_dims(np.array(cell_images), axis=-1).astype('float32')
        
        # 1. å†æ§‹æˆèª¤å·®
        reconstructed = self.autoencoder.predict(X, verbose=0)
        mse_errors = np.mean(np.square(X - reconstructed), axis=(1, 2, 3))
        mae_errors = np.mean(np.abs(X - reconstructed), axis=(1, 2, 3))
        
        # 2. ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ç‰¹å¾´é‡ãƒ™ãƒ¼ã‚¹
        encoded_features = self.encoder.predict(X, verbose=0)
        encoded_flat = encoded_features.reshape(len(encoded_features), -1)
        
        # å‰å‡¦ç†ï¼ˆè¨“ç·´æ™‚ã¨åŒã˜ï¼‰
        encoded_scaled = self.scaler.transform(encoded_flat)
        encoded_pca = self.pca.transform(encoded_scaled)
        
        # å…¨æ¤œçŸ¥å™¨ã§ã®ç•°å¸¸æ¤œçŸ¥
        results = {
            'reconstruction_mse': mse_errors,
            'reconstruction_mae': mae_errors,
        }
        
        for detector_name, detector in self.detectors.items():
            predictions = detector.predict(encoded_pca)
            scores = detector.decision_function(encoded_pca)
            
            results[f'{detector_name}_predictions'] = predictions
            results[f'{detector_name}_scores'] = -scores  # é«˜ã„ã»ã©ç•°å¸¸
            results[f'{detector_name}_anomaly_rate'] = np.sum(predictions == -1) / len(predictions)
        
        return results
    
    def compute_threshold_based_anomaly_scores(self, cell_images):
        """é–¾å€¤ãƒ™ãƒ¼ã‚¹ã®å‹•çš„ç•°å¸¸æ¤œçŸ¥"""
        if len(cell_images) == 0:
            return {}
        
        X = np.expand_dims(np.array(cell_images), axis=-1).astype('float32')
        
        # 1. å†æ§‹æˆèª¤å·®ã«ã‚ˆã‚‹æ¤œçŸ¥
        reconstructed = self.autoencoder.predict(X, verbose=0)
        mse_errors = np.mean(np.square(X - reconstructed), axis=(1, 2, 3))
        mae_errors = np.mean(np.abs(X - reconstructed), axis=(1, 2, 3))
        
        # 2. ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ç‰¹å¾´é‡ãƒ™ãƒ¼ã‚¹
        encoded_features = self.encoder.predict(X, verbose=0)
        encoded_flat = encoded_features.reshape(len(encoded_features), -1)
        encoded_scaled = self.scaler.transform(encoded_flat)
        encoded_pca = self.pca.transform(encoded_scaled)
        
        # 3. å‹•çš„é–¾å€¤ã®è¨ˆç®—
        results = {
            'reconstruction_mse': mse_errors,
            'reconstruction_mae': mae_errors,
        }
        
        # å„æ¤œçŸ¥å™¨ã§ã®ã‚¹ã‚³ã‚¢è¨ˆç®—
        for detector_name, detector in self.detectors.items():
            scores = detector.decision_function(encoded_pca)
            results[f'{detector_name}_scores'] = -scores  # é«˜ã„ã»ã©ç•°å¸¸
        
        # 4. ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«é–¾å€¤ã«ã‚ˆã‚‹ç•°å¸¸åˆ¤å®š
        percentile_thresholds = [99.9, 99.5, 99.0, 95.0, 90.0, 80.0, 70.0, 60.0, 50.0]
        
        for detector_name in self.detectors.keys():
            score_key = f'{detector_name}_scores'
            if score_key in results:
                scores = results[score_key]
                
                for percentile in percentile_thresholds:
                    threshold = np.percentile(scores, percentile)
                    anomaly_predictions = scores > threshold
                    anomaly_rate = np.sum(anomaly_predictions) / len(anomaly_predictions)
                    
                    results[f'{detector_name}_p{int(percentile)}_predictions'] = anomaly_predictions
                    results[f'{detector_name}_p{int(percentile)}_anomaly_rate'] = anomaly_rate
                    results[f'{detector_name}_p{int(percentile)}_threshold'] = threshold
        
        # 5. å†æ§‹æˆèª¤å·®é–¾å€¤
        for error_type in ['mse', 'mae']:
            errors = results[f'reconstruction_{error_type}']
            
            for percentile in percentile_thresholds:
                threshold = np.percentile(errors, percentile)
                anomaly_predictions = errors > threshold
                anomaly_rate = np.sum(anomaly_predictions) / len(anomaly_predictions)
                
                results[f'reconstruction_{error_type}_p{int(percentile)}_predictions'] = anomaly_predictions
                results[f'reconstruction_{error_type}_p{int(percentile)}_anomaly_rate'] = anomaly_rate
                results[f'reconstruction_{error_type}_p{int(percentile)}_threshold'] = threshold
        
        return results
    
    def adaptive_anomaly_detection(self, cell_images, target_sensitivity=0.90):
        """é©å¿œçš„ç•°å¸¸æ¤œçŸ¥ï¼ˆç›®æ¨™æ„Ÿåº¦ã‚’æŒ‡å®šï¼‰"""
        if len(cell_images) == 0:
            return {}
        
        # å…¨ã‚¹ã‚³ã‚¢è¨ˆç®—
        all_scores = self.compute_threshold_based_anomaly_scores(cell_images)
        
        # æœ€é©é–¾å€¤ã®é¸æŠ
        best_results = {}
        
        for detector_name in self.detectors.keys():
            score_key = f'{detector_name}_scores'
            if score_key in all_scores:
                scores = all_scores[score_key]
                
                # ç›®æ¨™æ„Ÿåº¦ã«æœ€ã‚‚è¿‘ã„é–¾å€¤ã‚’é¸æŠ
                best_percentile = None
                best_rate = None
                target_rate = target_sensitivity
                
                percentile_thresholds = [99.9, 99.5, 99.0, 95.0, 90.0, 80.0, 70.0, 60.0, 50.0]
                
                for percentile in percentile_thresholds:
                    rate_key = f'{detector_name}_p{int(percentile)}_anomaly_rate'
                    if rate_key in all_scores:
                        rate = all_scores[rate_key]
                        
                        if best_rate is None or abs(rate - target_rate) < abs(best_rate - target_rate):
                            best_rate = rate
                            best_percentile = percentile
                
                if best_percentile is not None:
                    pred_key = f'{detector_name}_p{int(best_percentile)}_predictions'
                    threshold_key = f'{detector_name}_p{int(best_percentile)}_threshold'
                    
                    best_results[f'{detector_name}_adaptive_predictions'] = all_scores[pred_key]
                    best_results[f'{detector_name}_adaptive_anomaly_rate'] = best_rate
                    best_results[f'{detector_name}_adaptive_threshold'] = all_scores[threshold_key]
                    best_results[f'{detector_name}_adaptive_percentile'] = best_percentile
        
        # åŸºæœ¬æƒ…å ±ã‚‚å«ã‚ã‚‹
        best_results['reconstruction_mse'] = all_scores['reconstruction_mse']
        best_results['reconstruction_mae'] = all_scores['reconstruction_mae']
        
        return best_results
    
    def ensemble_anomaly_detection(self, cell_images, consensus_threshold=0.5):
        """è¤‡æ•°æ¤œçŸ¥å™¨ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æŠ•ç¥¨ã«ã‚ˆã‚‹ç•°å¸¸æ¤œçŸ¥"""
        if len(cell_images) == 0:
            return {}
        
        X = np.expand_dims(np.array(cell_images), axis=-1).astype('float32')
        
        # 1. å†æ§‹æˆèª¤å·®
        reconstructed = self.autoencoder.predict(X, verbose=0)
        mse_errors = np.mean(np.square(X - reconstructed), axis=(1, 2, 3))
        mae_errors = np.mean(np.abs(X - reconstructed), axis=(1, 2, 3))
        
        # 2. ç‰¹å¾´é‡æŠ½å‡ºã¨å‰å‡¦ç†
        encoded_features = self.encoder.predict(X, verbose=0)
        encoded_flat = encoded_features.reshape(len(encoded_features), -1)
        encoded_scaled = self.scaler.transform(encoded_flat)
        encoded_pca = self.pca.transform(encoded_scaled)
        
        # 3. å„æ¤œçŸ¥å™¨ã®æŠ•ç¥¨
        votes = []
        detector_results = {}
        
        for detector_name, detector in self.detectors.items():
            predictions = detector.predict(encoded_pca)
            scores = detector.decision_function(encoded_pca)
            
            # ç•°å¸¸(-1)ã‚’1ã«ã€æ­£å¸¸(+1)ã‚’0ã«å¤‰æ›
            anomaly_votes = (predictions == -1).astype(int)
            votes.append(anomaly_votes)
            
            detector_results[f'{detector_name}_predictions'] = predictions
            detector_results[f'{detector_name}_scores'] = -scores
            detector_results[f'{detector_name}_anomaly_rate'] = np.mean(anomaly_votes)
        
        # 4. å†æ§‹æˆèª¤å·®ã«ã‚ˆã‚‹æŠ•ç¥¨
        mse_threshold = np.percentile(mse_errors, 95)
        mae_threshold = np.percentile(mae_errors, 95)
        
        mse_votes = (mse_errors > mse_threshold).astype(int)
        mae_votes = (mae_errors > mae_threshold).astype(int)
        
        votes.extend([mse_votes, mae_votes])
        
        # 5. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æŠ•ç¥¨
        if len(votes) > 0:
            vote_matrix = np.array(votes).T  # (n_cells, n_voters)
            vote_counts = np.sum(vote_matrix, axis=1)  # å„ç´°èƒã®ç•°å¸¸ç¥¨æ•°
            total_voters = vote_matrix.shape[1]
            
            # æŠ•ç¥¨ç‡ã«ã‚ˆã‚‹ç•°å¸¸åˆ¤å®š
            vote_ratios = vote_counts / total_voters
            
            # è¤‡æ•°ã®é–¾å€¤ã§ã®åˆ¤å®š
            ensemble_results = {
                'reconstruction_mse': mse_errors,
                'reconstruction_mae': mae_errors,
                'vote_counts': vote_counts,
                'vote_ratios': vote_ratios,
                'total_voters': total_voters
            }
            
            # å„æ¤œçŸ¥å™¨ã®çµæœã‚‚å«ã‚ã‚‹
            ensemble_results.update(detector_results)
            
            # ç•°ãªã‚‹åˆæ„ãƒ¬ãƒ™ãƒ«ã§ã®åˆ¤å®š
            consensus_levels = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
            
            for level in consensus_levels:
                consensus_predictions = vote_ratios >= level
                consensus_rate = np.mean(consensus_predictions)
                
                ensemble_results[f'consensus_{int(level*100)}_predictions'] = consensus_predictions
                ensemble_results[f'consensus_{int(level*100)}_anomaly_rate'] = consensus_rate
            
            # æ¨å¥¨ãƒ¬ãƒ™ãƒ«ï¼ˆ50%åˆæ„ï¼‰
            recommended_predictions = vote_ratios >= consensus_threshold
            ensemble_results['recommended_predictions'] = recommended_predictions
            ensemble_results['recommended_anomaly_rate'] = np.mean(recommended_predictions)
            ensemble_results['recommended_threshold'] = consensus_threshold
            
            return ensemble_results
        
        return {}
    
    def comprehensive_screening_analysis(self, test_folders_dict, output_dir):
        """åŒ…æ‹¬çš„ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°è§£æ"""
        os.makedirs(output_dir, exist_ok=True)
        
        print("=== COMPREHENSIVE SCREENING ANALYSIS ===")
        
        all_results = {}
        
        for sample_name, folder_path in test_folders_dict.items():
            print(f"\nProcessing {sample_name}...")
            
            tif_files = sorted(glob(os.path.join(folder_path, '*.tif')))
            sample_cells = []
            
            for file_path in tif_files:
                cells, _ = self.extract_quality_cells(file_path)
                sample_cells.extend(cells)
            
            if len(sample_cells) == 0:
                print(f"  No cells extracted")
                continue
            
            print(f"  Analyzing {len(sample_cells)} cells...")
            
            # 1. æ¨™æº–æ¤œçŸ¥
            standard_results = self.compute_anomaly_scores(sample_cells)
            
            # 2. é©å¿œçš„æ¤œçŸ¥
            adaptive_results = self.adaptive_anomaly_detection(sample_cells, target_sensitivity=0.90)
            
            # 3. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ¤œçŸ¥
            ensemble_results = self.ensemble_anomaly_detection(sample_cells, consensus_threshold=0.5)
            
            # çµæœçµ±åˆ
            comprehensive_result = {
                'sample_name': sample_name,
                'total_cells': len(sample_cells),
                'standard': standard_results,
                'adaptive': adaptive_results,
                'ensemble': ensemble_results
            }
            
            all_results[sample_name] = comprehensive_result
            
            # çµæœè¡¨ç¤º
            print(f"  Results summary:")
            
            # æ¨™æº–æ¤œçŸ¥çµæœ
            if 'conservative_anomaly_rate' in standard_results:
                rate = standard_results['conservative_anomaly_rate'] * 100
                print(f"    Standard Conservative: {rate:.1f}%")
            
            # é©å¿œçš„æ¤œçŸ¥çµæœ
            for detector_name in ['extreme_conservative', 'ultra_conservative', 'conservative']:
                if detector_name in self.detectors:
                    rate_key = f'{detector_name}_adaptive_anomaly_rate'
                    if rate_key in adaptive_results:
                        rate = adaptive_results[rate_key] * 100
                        percentile = adaptive_results.get(f'{detector_name}_adaptive_percentile', 'N/A')
                        print(f"    Adaptive {detector_name}: {rate:.1f}% (P{percentile})")
            
            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«çµæœ
            if 'recommended_anomaly_rate' in ensemble_results:
                rate = ensemble_results['recommended_anomaly_rate'] * 100
                print(f"    Ensemble (50% consensus): {rate:.1f}%")
            
            # é«˜åˆæ„ãƒ¬ãƒ™ãƒ«
            if 'consensus_80_anomaly_rate' in ensemble_results:
                rate = ensemble_results['consensus_80_anomaly_rate'] * 100
                print(f"    Ensemble (80% consensus): {rate:.1f}%")
        
        # çµæœã®ä¿å­˜
        self.save_comprehensive_results(all_results, output_dir)
        
        return all_results
    
    def save_comprehensive_results(self, all_results, output_dir):
        """åŒ…æ‹¬çš„çµæœã®ä¿å­˜"""
        
        # ã‚µãƒãƒªãƒ¼ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
        summary_data = []
        
        for sample_name, result in all_results.items():
            row = {
                'Sample': sample_name,
                'Total_Cells': result['total_cells']
            }
            
            # æ¨™æº–çµæœ
            standard = result.get('standard', {})
            for detector_name in self.detectors.keys():
                rate_key = f'{detector_name}_anomaly_rate'
                if rate_key in standard:
                    row[f'Standard_{detector_name}'] = standard[rate_key] * 100
            
            # é©å¿œçš„çµæœ
            adaptive = result.get('adaptive', {})
            for detector_name in self.detectors.keys():
                rate_key = f'{detector_name}_adaptive_anomaly_rate'
                if rate_key in adaptive:
                    row[f'Adaptive_{detector_name}'] = adaptive[rate_key] * 100
            
            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«çµæœ
            ensemble = result.get('ensemble', {})
            if 'recommended_anomaly_rate' in ensemble:
                row['Ensemble_50pct'] = ensemble['recommended_anomaly_rate'] * 100
            if 'consensus_80_anomaly_rate' in ensemble:
                row['Ensemble_80pct'] = ensemble['consensus_80_anomaly_rate'] * 100
            if 'consensus_90_anomaly_rate' in ensemble:
                row['Ensemble_90pct'] = ensemble['consensus_90_anomaly_rate'] * 100
            
            summary_data.append(row)
        
        # CSVä¿å­˜
        summary_df = pd.DataFrame(summary_data)
        summary_df.to_csv(os.path.join(output_dir, 'comprehensive_screening_summary.csv'), index=False)
        
        # è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ
        with open(os.path.join(output_dir, 'comprehensive_screening_report.txt'), 'w') as f:
            f.write("=== COMPREHENSIVE SCREENING REPORT ===\n\n")
            f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("ANALYSIS METHODS:\n")
            f.write("1. Standard Detection: Original SVM-based detection\n")
            f.write("2. Adaptive Detection: Dynamic threshold selection for target sensitivity (90%)\n")
            f.write("3. Ensemble Detection: Multi-detector voting system\n\n")
            
            f.write("SCREENING RESULTS:\n")
            f.write("-" * 130 + "\n")
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼
            header = f"{'Sample':<15} {'Cells':<6}"
            header += f" {'Std_Cons':<10} {'Adapt_Ext':<11} {'Adapt_Ultra':<12} {'Ens_50%':<9} {'Ens_80%':<9} {'Ens_90%':<9}"
            f.write(header + "\n")
            f.write("-" * 130 + "\n")
            
            # ãƒ‡ãƒ¼ã‚¿è¡Œ
            for _, row in summary_df.iterrows():
                line = f"{row['Sample']:<15} {row['Total_Cells']:<6}"
                
                # å„åˆ—ã®ãƒ‡ãƒ¼ã‚¿ã‚’å®‰å…¨ã«å–å¾—
                std_cons = row.get('Standard_conservative', 0)
                adapt_ext = row.get('Adaptive_extreme_conservative', 0)
                adapt_ultra = row.get('Adaptive_ultra_conservative', 0) 
                ens_50 = row.get('Ensemble_50pct', 0)
                ens_80 = row.get('Ensemble_80pct', 0)
                ens_90 = row.get('Ensemble_90pct', 0)
                
                line += f" {std_cons:>8.1f}% {adapt_ext:>9.1f}% {adapt_ultra:>10.1f}% {ens_50:>7.1f}% {ens_80:>7.1f}% {ens_90:>7.1f}%"
                f.write(line + "\n")
            
            f.write("\n")
            
            # é«˜ç•°å¸¸ç‡å€™è£œã®ç‰¹å®š
            f.write("HIGH ANOMALY CANDIDATES:\n")
            
            # 90%ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«åŸºæº–
            high_ensemble_90 = summary_df[summary_df.get('Ensemble_90pct', 0) > 50]
            if not high_ensemble_90.empty:
                f.write("Based on 90% ensemble consensus (>50%):\n")
                for _, row in high_ensemble_90.iterrows():
                    f.write(f"ğŸ”¥ {row['Sample']}: {row.get('Ensemble_90pct', 0):.1f}% - EXTREMELY HIGH CONFIDENCE\n")
                f.write("\n")
            
            # 80%ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«åŸºæº–
            high_ensemble_80 = summary_df[summary_df.get('Ensemble_80pct', 0) > 30]
            if not high_ensemble_80.empty:
                f.write("Based on 80% ensemble consensus (>30%):\n")
                for _, row in high_ensemble_80.iterrows():
                    f.write(f"âš ï¸  {row['Sample']}: {row.get('Ensemble_80pct', 0):.1f}% - HIGH CONFIDENCE\n")
                f.write("\n")
            
            # é©å¿œçš„æ¤œçŸ¥åŸºæº–
            adapt_ultra_high = summary_df[summary_df.get('Adaptive_ultra_conservative', 0) > 70]
            if not adapt_ultra_high.empty:
                f.write("Based on Adaptive Ultra-Conservative (>70%):\n")
                for _, row in adapt_ultra_high.iterrows():
                    f.write(f"ğŸ“‹ {row['Sample']}: {row.get('Adaptive_ultra_conservative', 0):.1f}% - ADAPTIVE HIGH\n")
                f.write("\n")
            
            if high_ensemble_90.empty and high_ensemble_80.empty and adapt_ultra_high.empty:
                f.write("No samples exceeded high-confidence thresholds.\n")
                
                # ä¸­ç¨‹åº¦ã®å€™è£œã‚‚è¡¨ç¤º
                moderate_candidates = summary_df[
                    (summary_df.get('Ensemble_50pct', 0) > 20) |
                    (summary_df.get('Adaptive_ultra_conservative', 0) > 50)
                ]
                
                if not moderate_candidates.empty:
                    f.write("\nMODERATE ANOMALY CANDIDATES:\n")
                    for _, row in moderate_candidates.iterrows():
                        ens_50 = row.get('Ensemble_50pct', 0)
                        adapt_ultra = row.get('Adaptive_ultra_conservative', 0)
                        f.write(f"â€¢ {row['Sample']}: Ens50={ens_50:.1f}%, AdaptUltra={adapt_ultra:.1f}%\n")
            
            f.write("\nRECOMMENDATIONS:\n")
            f.write("1. Prioritize samples with 90% ensemble consensus >50%\n")
            f.write("2. Validate samples with 80% ensemble consensus >30%\n")
            f.write("3. Consider adaptive ultra-conservative results >70%\n")
            f.write("4. Use multiple detection methods for confidence assessment\n")
            f.write("5. Perform biological validation for all high-confidence candidates\n")
        
        print(f"Comprehensive results saved to: {output_dir}")
    
    def screen_mutant_samples(self, test_folders_dict, output_dir):
        """æ¨™æº–çš„ãªã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ï¼‰"""
        return self.comprehensive_screening_analysis(test_folders_dict, output_dir)

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°ï¼ˆåŒ…æ‹¬çš„è§£æå¯¾å¿œï¼‰"""
    # è¨­å®š
    model_dir = "/Users/matsuokoujirou/Documents/Data/Screening/Models/20250613_1909_ultra_sensitive"
    
    # ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«
    test_folders = {
        'RG2_current': "/Users/matsuokoujirou/Documents/Data/Screening/250603_check/RG2",
        "EPYC1":"/Users/matsuokoujirou/Documents/Data/Screening/Abnormal_data/epyc1",
        "MITH1":"/Users/matsuokoujirou/Documents/Data/Screening/Abnormal_data/mith1",
        "RBMP1":"/Users/matsuokoujirou/Documents/Data/Screening/Abnormal_data/rbmp1",
        "RBMP2":"/Users/matsuokoujirou/Documents/Data/Screening/Abnormal_data/rbmp2",
        "RG2_AM":"/Users/matsuokoujirou/Documents/Data/Screening/Abnormal_data/RG2",
        "SAGA1":"/Users/matsuokoujirou/Documents/Data/Screening/Abnormal_data/saga1",
        "SAGA2":"/Users/matsuokoujirou/Documents/Data/Screening/Abnormal_data/saga2",
    }
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    output_dir = f"/Users/matsuokoujirou/Documents/Data/Screening/Results/{datetime.now().strftime('%Y%m%d_%H%M')}_comprehensive_screening"
    
    # åŒ…æ‹¬çš„ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
    screener = ProductionMutantScreening(model_dir)
    all_results = screener.comprehensive_screening_analysis(test_folders, output_dir)
    
    print(f"\n=== COMPREHENSIVE SCREENING COMPLETED ===")
    print(f"Results saved to: {output_dir}")
    
    # æœ€çµ‚ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    print(f"\n=== FINAL SUMMARY ===")
    
    high_confidence_found = False
    
    for sample_name, result in all_results.items():
        ensemble = result.get('ensemble', {})
        adaptive = result.get('adaptive', {})
        
        # é«˜ä¿¡é ¼åº¦å€™è£œã®ç‰¹å®š
        ens_90 = ensemble.get('consensus_90_anomaly_rate', 0) * 100
        ens_80 = ensemble.get('consensus_80_anomaly_rate', 0) * 100
        ens_50 = ensemble.get('recommended_anomaly_rate', 0) * 100
        
        adapt_ultra = adaptive.get('ultra_conservative_adaptive_anomaly_rate', 0) * 100
        adapt_extreme = adaptive.get('extreme_conservative_adaptive_anomaly_rate', 0) * 100
        
        # çµæœåˆ†é¡
        if ens_90 > 50 or adapt_extreme > 80:
            print(f"ğŸ”¥ {sample_name}: EXTREMELY HIGH ANOMALY")
            print(f"   90% Consensus: {ens_90:.1f}%, Adaptive Extreme: {adapt_extreme:.1f}%")
            high_confidence_found = True
        elif ens_80 > 30 or adapt_ultra > 70:
            print(f"âš ï¸  {sample_name}: HIGH ANOMALY")
            print(f"   80% Consensus: {ens_80:.1f}%, Adaptive Ultra: {adapt_ultra:.1f}%")
            high_confidence_found = True
        elif ens_50 > 20 or adapt_ultra > 50:
            print(f"ğŸ“‹ {sample_name}: MODERATE ANOMALY")
            print(f"   50% Consensus: {ens_50:.1f}%, Adaptive Ultra: {adapt_ultra:.1f}%")
        else:
            print(f"âœ… {sample_name}: NORMAL")
            print(f"   50% Consensus: {ens_50:.1f}%, Adaptive Ultra: {adapt_ultra:.1f}%")
    
    if high_confidence_found:
        print(f"\nğŸ¯ HIGH-CONFIDENCE ANOMALIES DETECTED!")
        print(f"   Recommend immediate biological validation.")
    else:
        print(f"\nâš¡ No high-confidence anomalies detected.")
        print(f"   Consider reviewing moderate candidates or adjusting sensitivity.")

def run_adaptive_screening_example():
    """é©å¿œçš„ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã®å®Ÿè¡Œä¾‹"""
    
    model_dir = "/Users/matsuokoujirou/Documents/Data/Screening/Models/20250613_1524_improved"
    
    test_folders = {
        'RG2_current': "/Users/matsuokoujirou/Documents/Data/Screening/250603_check/RG2",
        "SAGA2":"/Users/matsuokoujirou/Documents/Data/Screening/Abnormal_data/saga2",
        "RBMP1":"/Users/matsuokoujirou/Documents/Data/Screening/Abnormal_data/rbmp1",
    }
    
    screener = ProductionMutantScreening(model_dir)
    
    print("=== ADAPTIVE SCREENING RESULTS ===")
    
    for sample_name, folder_path in test_folders.items():
        print(f"\nProcessing {sample_name}...")
        
        tif_files = sorted(glob(os.path.join(folder_path, '*.tif')))
        sample_cells = []
        
        for file_path in tif_files:
            cells, _ = screener.extract_quality_cells(file_path)
            sample_cells.extend(cells)
        
        if len(sample_cells) == 0:
            continue
        
        # é©å¿œçš„ç•°å¸¸æ¤œçŸ¥
        adaptive_results = screener.adaptive_anomaly_detection(
            sample_cells, 
            target_sensitivity=0.90  # 90%ã®ç•°å¸¸æ¤œçŸ¥ã‚’ç›®æ¨™
        )
        
        print(f"  Total cells: {len(sample_cells)}")
        print(f"  Adaptive anomaly rates:")
        
        for detector_name in screener.detectors.keys():
            rate_key = f'{detector_name}_adaptive_anomaly_rate'
            percentile_key = f'{detector_name}_adaptive_percentile'
            
            if rate_key in adaptive_results:
                rate = adaptive_results[rate_key] * 100
                percentile = adaptive_results[percentile_key]
                print(f"    {detector_name}: {rate:.1f}% (using {percentile}th percentile)")

class PooledMutantScreening(ProductionMutantScreening):
    """ãƒ—ãƒ¼ãƒ«è§£æå¯¾å¿œã®ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ """
    
    def create_pools(self, colony_paths, pool_size=24):
        """ã‚³ãƒ­ãƒ‹ãƒ¼ã‚’ãƒ—ãƒ¼ãƒ«ã«åˆ†å‰²"""
        pools = []
        for i in range(0, len(colony_paths), pool_size):
            pool = colony_paths[i:i+pool_size]
            pools.append({
                'pool_id': f'Pool_{i//pool_size + 1:03d}',
                'colonies': pool,
                'colony_names': [os.path.basename(p) for p in pool]
            })
        return pools
    
    def screen_pool(self, pool_colonies, detection_method='adaptive'):
        """ãƒ—ãƒ¼ãƒ«å…¨ä½“ã®ç•°å¸¸ç‡ã‚’æ¸¬å®š"""
        all_cells = []
        colony_info = []
        
        print(f"  Processing {len(pool_colonies)} colonies in pool...")
        
        for colony_path in pool_colonies:
            cells, stats = self.extract_quality_cells(colony_path)
            all_cells.extend(cells)
            
            colony_info.append({
                'colony': os.path.basename(colony_path),
                'cell_count': len(cells),
                'path': colony_path
            })
        
        if len(all_cells) == 0:
            return {
                'total_cells': 0,
                'colony_info': colony_info,
                'anomaly_rate': 0.0,
                'status': 'no_cells'
            }
        
        # ãƒ—ãƒ¼ãƒ«å…¨ä½“ã®ç•°å¸¸ç‡ï¼ˆé¸æŠã•ã‚ŒãŸæ–¹æ³•ã§ï¼‰
        if detection_method == 'adaptive':
            pool_results = self.adaptive_anomaly_detection(all_cells, target_sensitivity=0.90)
            # æœ€ã‚‚å³æ ¼ãªæ¤œçŸ¥å™¨ã®çµæœã‚’ä½¿ç”¨
            primary_detector = 'ultra_conservative'
            if primary_detector in self.detectors:
                rate_key = f'{primary_detector}_adaptive_anomaly_rate'
                anomaly_rate = pool_results.get(rate_key, 0.0)
            else:
                anomaly_rate = 0.0
        elif detection_method == 'ensemble':
            pool_results = self.ensemble_anomaly_detection(all_cells, consensus_threshold=0.8)
            anomaly_rate = pool_results.get('consensus_80_anomaly_rate', 0.0)
        else:  # standard
            pool_results = self.compute_anomaly_scores(all_cells)
            anomaly_rate = pool_results.get('conservative_anomaly_rate', 0.0)
        
        return {
            'total_cells': len(all_cells),
            'colony_info': colony_info,
            'anomaly_rate': anomaly_rate,
            'all_scores': pool_results,
            'status': 'success'
        }
    
    def hierarchical_screening(self, colony_paths, pool_size=24, 
                             detection_method='adaptive',
                             threshold=0.20):  # 20%é–¾å€¤
        """éšå±¤çš„ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ"""
        
        print(f"=== HIERARCHICAL POOLED SCREENING ===")
        print(f"Total colonies: {len(colony_paths)}")
        print(f"Pool size: {pool_size}")
        print(f"Detection method: {detection_method}")
        print(f"Threshold: {threshold*100:.1f}%")
        
        # Step 1: ãƒ—ãƒ¼ãƒ«ä½œæˆ
        pools = self.create_pools(colony_paths, pool_size)
        print(f"Created {len(pools)} pools")
        
        pool_results = []
        high_anomaly_pools = []
        
        # Step 2: ãƒ—ãƒ¼ãƒ«ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        print(f"\n--- Pool Screening Phase ---")
        for i, pool in enumerate(pools):
            print(f"Screening {pool['pool_id']} ({len(pool['colonies'])} colonies)...")
            
            pool_result = self.screen_pool(pool['colonies'], detection_method)
            
            if pool_result['status'] == 'no_cells':
                print(f"  No cells extracted - skipping")
                continue
                
            anomaly_rate = pool_result['anomaly_rate']
            
            pool_summary = {
                'pool_id': pool['pool_id'],
                'anomaly_rate': anomaly_rate,
                'cell_count': pool_result['total_cells'],
                'colonies': pool['colonies'],
                'colony_names': pool['colony_names'],
                'detailed_result': pool_result
            }
            
            pool_results.append(pool_summary)
            
            # é«˜ç•°å¸¸ç‡ãƒ—ãƒ¼ãƒ«ã‚’ç‰¹å®š
            if anomaly_rate > threshold:
                high_anomaly_pools.append(pool_summary)
                print(f"  ğŸš¨ HIGH ANOMALY: {anomaly_rate*100:.2f}%")
            else:
                print(f"  âœ… Normal: {anomaly_rate*100:.2f}%")
        
        # Step 3: é«˜ç•°å¸¸ç‡ãƒ—ãƒ¼ãƒ«ã®å€‹åˆ¥è§£æ
        individual_results = []
        
        if high_anomaly_pools:
            print(f"\n--- Individual Analysis Phase ---")
            print(f"Analyzing {len(high_anomaly_pools)} high-anomaly pools individually...")
            
            for pool_summary in high_anomaly_pools:
                print(f"\nDetailed analysis of {pool_summary['pool_id']}...")
                
                for colony_path in pool_summary['colonies']:
                    colony_name = os.path.basename(colony_path)
                    cells, stats = self.extract_quality_cells(colony_path)
                    
                    if len(cells) > 0:
                        # åŒ…æ‹¬çš„è§£æ
                        standard_scores = self.compute_anomaly_scores(cells)
                        adaptive_scores = self.adaptive_anomaly_detection(cells, target_sensitivity=0.90)
                        ensemble_scores = self.ensemble_anomaly_detection(cells, consensus_threshold=0.8)
                        
                        # çµæœçµ±åˆ
                        individual_result = {
                            'colony': colony_name,
                            'colony_path': colony_path,
                            'pool_id': pool_summary['pool_id'],
                            'cell_count': len(cells)
                        }
                        
                        # å„æ¤œçŸ¥æ–¹æ³•ã®çµæœã‚’è¿½åŠ 
                        # æ¨™æº–æ¤œçŸ¥
                        for detector_name in self.detectors.keys():
                            rate_key = f'{detector_name}_anomaly_rate'
                            if rate_key in standard_scores:
                                individual_result[f'std_{rate_key}'] = standard_scores[rate_key]
                        
                        # é©å¿œçš„æ¤œçŸ¥
                        for detector_name in self.detectors.keys():
                            rate_key = f'{detector_name}_adaptive_anomaly_rate'
                            if rate_key in adaptive_scores:
                                individual_result[f'adapt_{rate_key}'] = adaptive_scores[rate_key]
                        
                        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ¤œçŸ¥
                        if 'consensus_80_anomaly_rate' in ensemble_scores:
                            individual_result['ensemble_80_anomaly_rate'] = ensemble_scores['consensus_80_anomaly_rate']
                        if 'recommended_anomaly_rate' in ensemble_scores:
                            individual_result['ensemble_50_anomaly_rate'] = ensemble_scores['recommended_anomaly_rate']
                        
                        individual_results.append(individual_result)
                        
                        # ä¸»è¦çµæœã®è¡¨ç¤º
                        primary_rate = adaptive_scores.get('ultra_conservative_adaptive_anomaly_rate', 0)
                        ensemble_rate = ensemble_scores.get('consensus_80_anomaly_rate', 0)
                        
                        if primary_rate > 0.8 or ensemble_rate > 0.6:  # 80%ã¾ãŸã¯60%é–¾å€¤
                            print(f"  ğŸ”¥ {colony_name}: Adapt={primary_rate*100:.1f}%, Ens80={ensemble_rate*100:.1f}% (VERY HIGH)")
                        elif primary_rate > 0.5 or ensemble_rate > 0.3:
                            print(f"  âš ï¸  {colony_name}: Adapt={primary_rate*100:.1f}%, Ens80={ensemble_rate*100:.1f}% (HIGH)")
                        elif primary_rate > 0.2 or ensemble_rate > 0.1:
                            print(f"  ğŸ“‹ {colony_name}: Adapt={primary_rate*100:.1f}%, Ens80={ensemble_rate*100:.1f}% (Moderate)")
                        else:
                            print(f"  âœ… {colony_name}: Adapt={primary_rate*100:.1f}%, Ens80={ensemble_rate*100:.1f}% (Normal)")
                    else:
                        print(f"  âŒ {colony_name}: No cells extracted")
        else:
            print(f"\nâœ… No high-anomaly pools detected!")
        
        # çµæœã‚µãƒãƒªãƒ¼
        self.save_pooled_results(pool_results, individual_results, detection_method, threshold)
        
        return pool_results, individual_results
    
    def save_pooled_results(self, pool_results, individual_results, detection_method, threshold):
        """ãƒ—ãƒ¼ãƒ«è§£æçµæœã®ä¿å­˜"""
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M')
        output_dir = f"/Users/matsuokoujirou/Documents/Data/Screening/Results/{timestamp}_pooled_screening"
        os.makedirs(output_dir, exist_ok=True)
        
        # ãƒ—ãƒ¼ãƒ«çµæœã®ä¿å­˜
        pool_df = pd.DataFrame([{
            'pool_id': r['pool_id'],
            'anomaly_rate': r['anomaly_rate'],
            'cell_count': r['cell_count'],
            'colony_count': len(r['colonies']),
            'status': 'HIGH' if r['anomaly_rate'] > threshold else 'NORMAL'
        } for r in pool_results])
        
        pool_df.to_csv(os.path.join(output_dir, 'pool_screening_results.csv'), index=False)
        
        # å€‹åˆ¥çµæœã®ä¿å­˜
        if individual_results:
            individual_df = pd.DataFrame(individual_results)
            individual_df.to_csv(os.path.join(output_dir, 'individual_analysis_results.csv'), index=False)
            
            # è¶…é«˜ç•°å¸¸ç‡ã‚³ãƒ­ãƒ‹ãƒ¼ã®ç‰¹å®š
            if 'adapt_ultra_conservative_adaptive_anomaly_rate' in individual_df.columns:
                super_high_colonies = individual_df[
                    individual_df['adapt_ultra_conservative_adaptive_anomaly_rate'] > 0.8
                ].sort_values('adapt_ultra_conservative_adaptive_anomaly_rate', ascending=False)
                
                super_high_colonies.to_csv(
                    os.path.join(output_dir, 'super_high_anomaly_colonies.csv'), index=False
                )
        
        # ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ
        with open(os.path.join(output_dir, 'pooled_screening_report.txt'), 'w') as f:
            f.write("=== POOLED MUTANT SCREENING REPORT ===\n\n")
            f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Detection method: {detection_method}\n")
            f.write(f"Threshold: {threshold*100:.1f}%\n\n")
            
            f.write("POOL SCREENING SUMMARY:\n")
            f.write(f"Total pools: {len(pool_results)}\n")
            high_pools = [r for r in pool_results if r['anomaly_rate'] > threshold]
            f.write(f"High-anomaly pools: {len(high_pools)}\n")
            f.write(f"Normal pools: {len(pool_results) - len(high_pools)}\n\n")
            
            if high_pools:
                f.write("HIGH-ANOMALY POOLS:\n")
                for pool in sorted(high_pools, key=lambda x: x['anomaly_rate'], reverse=True):
                    f.write(f"- {pool['pool_id']}: {pool['anomaly_rate']*100:.2f}% "
                           f"({pool['cell_count']} cells, {len(pool['colonies'])} colonies)\n")
            
            if individual_results:
                f.write(f"\nINDIVIDUAL ANALYSIS:\n")
                f.write(f"Colonies analyzed: {len(individual_results)}\n")
                
                individual_df = pd.DataFrame(individual_results)
                
                # è¶…é«˜ç•°å¸¸ç‡
                if 'adapt_ultra_conservative_adaptive_anomaly_rate' in individual_df.columns:
                    super_high = individual_df[individual_df['adapt_ultra_conservative_adaptive_anomaly_rate'] > 0.8]
                    f.write(f"Super-high anomaly (>80%): {len(super_high)}\n")
                    
                    for _, row in super_high.iterrows():
                        rate = row['adapt_ultra_conservative_adaptive_anomaly_rate']
                        f.write(f"  - {row['colony']}: {rate*100:.1f}%\n")
        
        print(f"\nResults saved to: {output_dir}")

def run_pooled_screening_example():
    """ãƒ—ãƒ¼ãƒ«è§£æã®å®Ÿè¡Œä¾‹"""
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    model_dir = "/Users/matsuokoujirou/Documents/Data/Screening/Models/20250613_1524_improved"
    
    # ã‚³ãƒ­ãƒ‹ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆä¾‹ï¼š240å€‹ã®ã‚³ãƒ­ãƒ‹ãƒ¼ï¼‰
    colony_base_path = "/path/to/colonies"
    colony_paths = sorted(glob(os.path.join(colony_base_path, "*.tif")))
    
    if len(colony_paths) == 0:
        print("No colony files found!")
        print("Please update colony_base_path in the function")
        return
    
    print(f"Found {len(colony_paths)} colony files")
    
    # ãƒ—ãƒ¼ãƒ«ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
    screener = PooledMutantScreening(model_dir)
    
    pool_results, individual_results = screener.hierarchical_screening(
        colony_paths,
        pool_size=24,                    # 24ã‚³ãƒ­ãƒ‹ãƒ¼/ãƒ—ãƒ¼ãƒ«
        detection_method='adaptive',     # é©å¿œçš„æ¤œçŸ¥ä½¿ç”¨
        threshold=0.20                   # 20%é–¾å€¤
    )
    
    # çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º
    print(f"\n=== FINAL SUMMARY ===")
    print(f"Pools screened: {len(pool_results)}")
    
    high_pools = [r for r in pool_results if r['anomaly_rate'] > 0.20]
    print(f"High-anomaly pools: {len(high_pools)}")
    
    if individual_results:
        print(f"Individual colonies analyzed: {len(individual_results)}")
        
        # æœ€é«˜ç•°å¸¸ç‡ã®ã‚³ãƒ­ãƒ‹ãƒ¼
        individual_df = pd.DataFrame(individual_results)
        if 'adapt_ultra_conservative_adaptive_anomaly_rate' in individual_df.columns:
            max_idx = individual_df['adapt_ultra_conservative_adaptive_anomaly_rate'].idxmax()
            top_colony = individual_df.loc[max_idx]
            rate = top_colony['adapt_ultra_conservative_adaptive_anomaly_rate']
            print(f"Highest anomaly colony: {top_colony['colony']} ({rate*100:.1f}%)")

if __name__ == "__main__":
    # é€šå¸¸ã®åŒ…æ‹¬çš„ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
    main()
    
    # é©å¿œçš„ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã®ä¾‹
    # run_adaptive_screening_example()
    
    # ãƒ—ãƒ¼ãƒ«è§£æã®ä¾‹
    # run_pooled_screening_example()